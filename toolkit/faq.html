

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Frequently Asked Questions &mdash; Cloud Native Products  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Platform support Information" href="platform.html" />
    <link rel="prev" title="Advanced Usage" href="advanced-usage.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Cloud Native Products
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">NVIDIA Container Toolkit</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#what-is-docker">What is Docker?</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#benefits-of-gpu-containerization">Benefits of GPU containerization</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#background-of-the-nvidia-container-toolkit">Background of the NVIDIA Container Toolkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#prerequisites-of-the-nvidia-container-toolkit">Prerequisites of the NVIDIA Container Toolkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#installation-of-the-nvidia-container-toolkit">Installation of the NVIDIA Container Toolkit</a></li>
<li class="toctree-l2"><a class="reference internal" href="quickstart.html#usage-of-the-nvidia-container-toolkit">Usage of the NVIDIA Container Toolkit</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="container-images.html">Container images</a><ul>
<li class="toctree-l2"><a class="reference internal" href="container-images.html#using-cuda-images">Using CUDA images</a></li>
<li class="toctree-l2"><a class="reference internal" href="container-images.html#using-ngc-images">Using NGC images</a></li>
<li class="toctree-l2"><a class="reference internal" href="container-images.html#using-non-cuda-images">Using non-CUDA images</a></li>
<li class="toctree-l2"><a class="reference internal" href="container-images.html#writing-dockerfiles">Writing Dockerfiles</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="jetson.html">NVIDIA Container Runtime on Jetson (Beta)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="jetson.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="jetson.html#installation">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="jetson.html#hello-world">Hello-world!</a></li>
<li class="toctree-l2"><a class="reference internal" href="jetson.html#building-cuda-in-containers-on-jetson">Building CUDA in Containers on Jetson</a></li>
<li class="toctree-l2"><a class="reference internal" href="jetson.html#enabling-jetson-containers-on-an-x86-workstation-using-qemu">Enabling Jetson Containers on an x86 workstation (using qemu)</a></li>
<li class="toctree-l2"><a class="reference internal" href="jetson.html#building-jetson-containers-on-an-x86-workstation-using-qemu">Building Jetson Containers on an x86 workstation (using qemu)</a></li>
<li class="toctree-l2"><a class="reference internal" href="jetson.html#troubleshooting">Troubleshooting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="jetson.html#no-package-show-are-shown-in-dpkgs-output">No package show are shown in dpkg’s output</a></li>
<li class="toctree-l3"><a class="reference internal" href="jetson.html#nvidia-docker2-package-is-missing-from-dpkgs-output">nvidia-docker2 package is missing from dpkg’s output</a></li>
<li class="toctree-l3"><a class="reference internal" href="jetson.html#docker-info-doesnt-show-the-nvidia-runtime">Docker info doesn’t show the NVIDIA runtime</a></li>
<li class="toctree-l3"><a class="reference internal" href="jetson.html#generating-and-viewing-logs">Generating and viewing logs</a></li>
<li class="toctree-l3"><a class="reference internal" href="jetson.html#usr-local-cuda-is-readonly">/usr/local/cuda is readonly</a></li>
<li class="toctree-l3"><a class="reference internal" href="jetson.html#running-or-building-a-container-on-x86-using-qemu-binfmt-misc-is-failing">Running or building a container on x86 (using qemu+binfmt_misc) is failing</a></li>
<li class="toctree-l3"><a class="reference internal" href="jetson.html#mount-plugins">Mount Plugins</a></li>
<li class="toctree-l3"><a class="reference internal" href="jetson.html#supported-devices">Supported Devices</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="advanced-usage.html">Advanced Usage</a><ul>
<li class="toctree-l2"><a class="reference internal" href="advanced-usage.html#general-topics">General Topics</a></li>
<li class="toctree-l2"><a class="reference internal" href="advanced-usage.html#nvidia-mps">NVIDIA MPS</a></li>
<li class="toctree-l2"><a class="reference internal" href="advanced-usage.html#internals-of-the-nvidia-container-toolkit">Internals of the NVIDIA Container Toolkit</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Frequently Asked Questions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#general-questions">General Questions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#container-runtime">Container Runtime</a></li>
<li class="toctree-l2"><a class="reference internal" href="#container-images">Container images</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ecosystem-enablement">Ecosystem enablement</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="platform.html">Platform support Information</a><ul>
<li class="toctree-l2"><a class="reference internal" href="platform.html#linux-distribution-matrix">Linux Distribution Matrix</a></li>
<li class="toctree-l2"><a class="reference internal" href="platform.html#additional-support-information">Additional Support Information</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="deprecated.html">Deprecated Features, Software and Images</a><ul>
<li class="toctree-l2"><a class="reference internal" href="deprecated.html#version-1-0">Version 1.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="deprecated.html#version-2-0">Version 2.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="deprecated.html#nvidia-caffe">NVIDIA Caffe</a></li>
<li class="toctree-l2"><a class="reference internal" href="deprecated.html#nvidia-digits">NVIDIA DIGITS</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">NVIDIA GPU Operator</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../gpu-operator/testing.html">Quality Assurance</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../gpu-operator/testing.html#tested-platforms">Tested Platforms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-operator/testing.html#end-to-end-stories">End to End Stories</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-operator/testing.html#as-a-cluster-admin-i-want-to-be-able-to-install-the-gpu-operator-with-helm-kubernetes-ubuntu-and-docker">As a cluster admin, I want to be able to install the GPU Operator with helm, Kubernetes, Ubuntu and Docker.</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-operator/testing.html#as-a-cluster-admin-i-want-to-be-able-to-install-the-gpu-operator-with-helm-openshift-4-1-rhcos-and-crio">As a cluster admin, I want to be able to install the GPU Operator with helm, Openshift 4.1, RHCOS and CRIO.</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-operator/testing.html#as-a-cluster-admin-i-want-to-be-able-to-gather-gpu-metrics-after-installing-the-gpu-operator">As a cluster admin, I want to be able to gather GPU metrics after installing the GPU Operator.</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-operator/testing.html#ipmi-msghandler-isn-t-loaded">ipmi_msghandler isn’t loaded</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-operator/testing.html#tainted-nodes">Tainted Nodes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-operator/testing.html#as-a-cluster-admin-i-want-to-ensure-that-the-gpu-operator-doesn-t-deploy-a-failing-monitoring-container">As a cluster admin, I want to ensure that the GPU Operator doesn’t deploy a failing monitoring container.</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gpu-operator/testing.html#key-performance-indicator">Key Performance Indicator</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gpu-operator/testing.html#quality-assurance-score-card">Quality Assurance Score Card</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-operator/testing.html#performance-score-card">Performance Score Card</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-operator/testing.html#security-score-card">Security Score Card</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gpu-operator/testing.html#bill-of-materials-score-card">Bill of Materials Score Card</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">NVIDIA Driver Container</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../driver/readme.html">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../driver/readme.html#description-and-requirements">Description and Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../driver/readme.html#configuration">Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../driver/readme.html#examples">Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../driver/readme.html#id1">Quickstart</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../driver/readme.html#ubuntu-distributions">Ubuntu Distributions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../driver/readme.html#centos-distributions">Centos Distributions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../driver/readme.html#kubernetes-with-dockerd">Kubernetes with dockerd</a></li>
<li class="toctree-l2"><a class="reference internal" href="../driver/readme.html#tags-available">Tags available</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Cloud Native Products</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Frequently Asked Questions</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/toolkit/faq.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="frequently-asked-questions">
<h1>Frequently Asked Questions<a class="headerlink" href="#frequently-asked-questions" title="Permalink to this headline">¶</a></h1>
<div class="contents local topic" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#general-questions" id="id5">General Questions</a></p>
<ul>
<li><p><a class="reference internal" href="#how-do-i-install-the-nvidia-driver" id="id6">How do I install the NVIDIA driver?</a></p></li>
<li><p><a class="reference internal" href="#the-following-signatures-were-invalid-expkeysig" id="id7"><code class="docutils literal notranslate"><span class="pre">The</span> <span class="pre">following</span> <span class="pre">signatures</span> <span class="pre">were</span> <span class="pre">invalid:</span> <span class="pre">EXPKEYSIG</span></code></a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#container-runtime" id="id8">Container Runtime</a></p>
<ul>
<li><p><a class="reference internal" href="#do-you-support-opencl" id="id9">Do you support OpenCL?</a></p></li>
<li><p><a class="reference internal" href="#do-you-support-opengl" id="id10">Do you support OpenGL?</a></p></li>
<li><p><a class="reference internal" href="#do-you-support-vulkan" id="id11">Do you support Vulkan?</a></p></li>
<li><p><a class="reference internal" href="#do-you-support-cuda-multi-process-service-a-k-a-mps" id="id12">Do you support CUDA Multi Process Service (a.k.a. MPS)?</a></p></li>
<li><p><a class="reference internal" href="#do-you-support-running-a-gpu-accelerated-x-server-inside-the-container" id="id13">Do you support running a GPU-accelerated X server inside the container?</a></p></li>
<li><p><a class="reference internal" href="#do-you-support-sharing-a-gpu-between-multiple-containers" id="id14">Do you support sharing a GPU between multiple containers?</a></p></li>
<li><p><a class="reference internal" href="#do-you-support-limiting-the-gpu-resources-e-g-bandwidth-memory-cuda-cores-taken-by-a-container" id="id15">Do you support limiting the GPU resources (e.g. bandwidth, memory, CUDA cores) taken by a container?</a></p></li>
<li><p><a class="reference internal" href="#do-you-support-enforcing-exclusive-access-for-a-gpu" id="id16">Do you support enforcing exclusive access for a GPU?</a></p></li>
<li><p><a class="reference internal" href="#does-it-have-a-performance-impact-on-my-gpu-workload" id="id17">Does it have a performance impact on my GPU workload?</a></p></li>
<li><p><a class="reference internal" href="#unsatisfied-condition-cuda-x-y" id="id18"><code class="docutils literal notranslate"><span class="pre">unsatisfied</span> <span class="pre">condition:</span> <span class="pre">cuda</span> <span class="pre">&gt;=</span> <span class="pre">X.Y</span></code></a></p></li>
<li><p><a class="reference internal" href="#i-have-multiple-gpu-devices-how-can-i-isolate-them-between-my-containers" id="id19">I have multiple GPU devices, how can I isolate them between my containers?</a></p></li>
<li><p><a class="reference internal" href="#why-is-nvidia-smi-inside-the-container-not-listing-the-running-processes" id="id20">Why is <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> inside the container not listing the running processes?</a></p></li>
<li><p><a class="reference internal" href="#why-is-my-container-slow-to-start" id="id21">Why is my container slow to start?</a></p></li>
<li><p><a class="reference internal" href="#can-i-use-it-with-docker-in-docker-a-k-a-dind" id="id22">Can I use it with Docker-in-Docker (a.k.a. DinD)?</a></p></li>
<li><p><a class="reference internal" href="#why-is-my-application-inside-the-container-slow-to-initialize" id="id23">Why is my application inside the container slow to initialize?</a></p></li>
<li><p><a class="reference internal" href="#is-the-jit-cache-shared-between-containers" id="id24">Is the JIT cache shared between containers?</a></p></li>
<li><p><a class="reference internal" href="#what-is-causing-the-cuda-invalid-device-function-error" id="id25">What is causing the CUDA <code class="docutils literal notranslate"><span class="pre">invalid</span> <span class="pre">device</span> <span class="pre">function</span></code> error?</a></p></li>
<li><p><a class="reference internal" href="#why-do-i-get-insufficient-permissions-for-some-nvidia-smi-operations" id="id26">Why do I get <code class="docutils literal notranslate"><span class="pre">Insufficient</span> <span class="pre">Permissions</span></code> for some <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> operations?</a></p></li>
<li><p><a class="reference internal" href="#can-i-profile-and-debug-my-gpu-code-inside-a-container" id="id27">Can I profile and debug my GPU code inside a container?</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#container-images" id="id28">Container images</a></p>
<ul>
<li><p><a class="reference internal" href="#what-do-i-have-to-install-in-my-container-images" id="id29">What do I have to install in my container images?</a></p></li>
<li><p><a class="reference internal" href="#do-you-provide-official-docker-images" id="id30">Do you provide official Docker images?</a></p></li>
<li><p><a class="reference internal" href="#can-i-use-the-gpu-during-a-container-build-i-e-docker-build" id="id31">Can I use the GPU during a container build (i.e. <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">build</span></code>)?</a></p></li>
<li><p><a class="reference internal" href="#are-my-container-images-built-for-version-1-0-compatible-with-2-0-and-3-0" id="id32">Are my container images built for version 1.0 compatible with 2.0 and 3.0?</a></p></li>
<li><p><a class="reference internal" href="#how-do-i-link-against-driver-apis-at-build-time-e-g-libcuda-so-or-libnvidia-ml-so" id="id33">How do I link against driver APIs at build time (e.g. <code class="docutils literal notranslate"><span class="pre">libcuda.so</span></code> or <code class="docutils literal notranslate"><span class="pre">libnvidia-ml.so</span></code>)?</a></p></li>
<li><p><a class="reference internal" href="#the-official-cuda-images-are-too-big-what-do-i-do" id="id34">The official CUDA images are too big, what do I do?</a></p></li>
<li><p><a class="reference internal" href="#why-aren-t-cuda-10-images-working-with-nvidia-docker-v1" id="id35">Why aren’t CUDA 10 images working with nvidia-docker v1?</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#ecosystem-enablement" id="id36">Ecosystem enablement</a></p>
<ul>
<li><p><a class="reference internal" href="#do-you-support-docker-swarm-mode" id="id37">Do you support Docker Swarm mode?</a></p></li>
<li><p><a class="reference internal" href="#do-you-support-docker-compose" id="id38">Do you support Docker Compose?</a></p></li>
<li><p><a class="reference internal" href="#do-you-support-kubernetes" id="id39">Do you support Kubernetes?</a></p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="general-questions">
<h2><a class="toc-backref" href="#id5">General Questions</a><a class="headerlink" href="#general-questions" title="Permalink to this headline">¶</a></h2>
<div class="section" id="how-do-i-install-the-nvidia-driver">
<h3><a class="toc-backref" href="#id6">How do I install the NVIDIA driver?</a><a class="headerlink" href="#how-do-i-install-the-nvidia-driver" title="Permalink to this headline">¶</a></h3>
<p>The recommended way is to use your <a class="reference external" href="http://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#package-manager-installation">package manager</a> and install the <code class="docutils literal notranslate"><span class="pre">cuda-drivers</span></code> package (or equivalent).
When no packages are available, you should use an official <a class="reference external" href="http://www.nvidia.com/object/unix.html">“runfile”</a>.</p>
<p>Alternatively, and as a technology preview, the NVIDIA driver can be deployed through a container.</p>
</div>
<div class="section" id="the-following-signatures-were-invalid-expkeysig">
<h3><a class="toc-backref" href="#id7"><code class="docutils literal notranslate"><span class="pre">The</span> <span class="pre">following</span> <span class="pre">signatures</span> <span class="pre">were</span> <span class="pre">invalid:</span> <span class="pre">EXPKEYSIG</span></code></a><a class="headerlink" href="#the-following-signatures-were-invalid-expkeysig" title="Permalink to this headline">¶</a></h3>
<p>Make sure you fetched the latest GPG key from the repositories. Refer to the <a class="reference external" href="https://nvidia.github.io/nvidia-docker/">repository instructions</a> for your distribution.</p>
</div>
</div>
<div class="section" id="container-runtime">
<h2><a class="toc-backref" href="#id8">Container Runtime</a><a class="headerlink" href="#container-runtime" title="Permalink to this headline">¶</a></h2>
<div class="section" id="do-you-support-opencl">
<h3><a class="toc-backref" href="#id9">Do you support OpenCL?</a><a class="headerlink" href="#do-you-support-opencl" title="Permalink to this headline">¶</a></h3>
<p>Yes, we now provide images on <a class="reference external" href="https://hub.docker.com/r/nvidia/opencl/">DockerHub</a>.</p>
</div>
<div class="section" id="do-you-support-opengl">
<h3><a class="toc-backref" href="#id10">Do you support OpenGL?</a><a class="headerlink" href="#do-you-support-opengl" title="Permalink to this headline">¶</a></h3>
<p>Yes, <a class="reference external" href="https://devblogs.nvidia.com/parallelforall/egl-eye-opengl-visualization-without-x-server/">EGL</a> is supported for headless rendering, but this is a <strong>beta</strong> feature. There is no plan to support GLX in the near future.</p>
<p>Images are available at <a class="reference external" href="https://hub.docker.com/r/nvidia/opengl/">nvidia/opengl</a>. If you need CUDA+OpenGL, use <a class="reference external" href="https://hub.docker.com/r/nvidia/cudagl/">nvidia/cudagl</a>.
If you are a <a class="reference external" href="https://github.com/NVIDIA/nvidia-docker/wiki/NGC">NGC</a> subscriber and require GLX for your workflow, please fill out a <a class="reference external" href="https://devtalk.nvidia.com/default/board/221/feature-requests/">feature request</a> for support consideration.</p>
</div>
<div class="section" id="do-you-support-vulkan">
<h3><a class="toc-backref" href="#id11">Do you support Vulkan?</a><a class="headerlink" href="#do-you-support-vulkan" title="Permalink to this headline">¶</a></h3>
<p>No, Vulkan is not supported at the moment. However we plan on supporting this feature in the future.</p>
</div>
<div class="section" id="do-you-support-cuda-multi-process-service-a-k-a-mps">
<h3><a class="toc-backref" href="#id12">Do you support CUDA Multi Process Service (a.k.a. MPS)?</a><a class="headerlink" href="#do-you-support-cuda-multi-process-service-a-k-a-mps" title="Permalink to this headline">¶</a></h3>
<p>No, MPS is not supported at the moment. However we plan on supporting this feature in the future, and <a class="reference external" href="https://github.com/NVIDIA/nvidia-docker/issues/419">this issue</a> will be updated accordingly.</p>
</div>
<div class="section" id="do-you-support-running-a-gpu-accelerated-x-server-inside-the-container">
<h3><a class="toc-backref" href="#id13">Do you support running a GPU-accelerated X server inside the container?</a><a class="headerlink" href="#do-you-support-running-a-gpu-accelerated-x-server-inside-the-container" title="Permalink to this headline">¶</a></h3>
<p>No, running a X server inside the container is not supported at the moment and there is no plan to support it in the near future (see also <a class="reference external" href="#is-opengl-supported">OpenGL support</a>).</p>
</div>
<div class="section" id="do-you-support-sharing-a-gpu-between-multiple-containers">
<h3><a class="toc-backref" href="#id14">Do you support sharing a GPU between multiple containers?</a><a class="headerlink" href="#do-you-support-sharing-a-gpu-between-multiple-containers" title="Permalink to this headline">¶</a></h3>
<p>Yes. This is no different than sharing a GPU between multiple processes outside of containers.
Scheduling and compute preemption vary from one GPU architecture to another (e.g. CTA-level, instruction-level).</p>
</div>
<div class="section" id="do-you-support-limiting-the-gpu-resources-e-g-bandwidth-memory-cuda-cores-taken-by-a-container">
<h3><a class="toc-backref" href="#id15">Do you support limiting the GPU resources (e.g. bandwidth, memory, CUDA cores) taken by a container?</a><a class="headerlink" href="#do-you-support-limiting-the-gpu-resources-e-g-bandwidth-memory-cuda-cores-taken-by-a-container" title="Permalink to this headline">¶</a></h3>
<p>No. Your only option is to set the GPU clocks at a lower frequency before starting the container.</p>
</div>
<div class="section" id="do-you-support-enforcing-exclusive-access-for-a-gpu">
<h3><a class="toc-backref" href="#id16">Do you support enforcing exclusive access for a GPU?</a><a class="headerlink" href="#do-you-support-enforcing-exclusive-access-for-a-gpu" title="Permalink to this headline">¶</a></h3>
<p>This is not currently supported but you can enforce it:</p>
<ul class="simple">
<li><p>At the container orchestration layer (Kubernetes, Swarm, Mesos, Slurm…) since this is tied to resource allocation.</p></li>
<li><p>At the driver level by setting the <a class="reference external" href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-modes">compute mode</a> of the GPU.</p></li>
</ul>
</div>
<div class="section" id="does-it-have-a-performance-impact-on-my-gpu-workload">
<h3><a class="toc-backref" href="#id17">Does it have a performance impact on my GPU workload?</a><a class="headerlink" href="#does-it-have-a-performance-impact-on-my-gpu-workload" title="Permalink to this headline">¶</a></h3>
<p>No, usually the impact should be in the order of less than 1% and hardly noticeable.
However be aware of the following (list non exhaustive):</p>
<ul class="simple">
<li><p><em>GPU topology and CPU affinity</em> You can query it using <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span> <span class="pre">topo</span></code> and use <a class="reference external" href="https://docs.docker.com/engine/admin/resource_constraints/#cpu">Docker CPU sets</a> to pin CPU cores.</p></li>
<li><p><em>Compiling your code for your device architecture</em>Your container might be compiled for the wrong achitecture and could fallback to the JIT compilation of PTX code (refer to the <a class="reference external" href="http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#gpu-compilation">official documentation</a> for more information).
Note that you can express <a class="reference external" href="https://github.com/nvidia/nvidia-container-runtime#nvidia_require_">these constraints</a> in your container image.</p></li>
<li><p><em>Container I/O overhead</em>By default Docker containers rely on an overlay filesystem and bridged/NATed networking.
Depending on your workload this can be a bottleneck, we recommend using <a class="reference external" href="https://docs.docker.com/engine/admin/volumes/volumes/">Docker volumes</a> and experiment with different <a class="reference external" href="https://docs.docker.com/engine/userguide/networking/">Docker networks</a>.</p></li>
<li><p><em>Linux kernel accounting and security overhead</em>In rare cases, you may notice than some kernel subsystems induce overhead.
This will likely depend on your kernel version and can include things like: cgroups, LSMs, seccomp filters, netfilter…</p></li>
</ul>
</div>
<div class="section" id="unsatisfied-condition-cuda-x-y">
<h3><a class="toc-backref" href="#id18"><code class="docutils literal notranslate"><span class="pre">unsatisfied</span> <span class="pre">condition:</span> <span class="pre">cuda</span> <span class="pre">&gt;=</span> <span class="pre">X.Y</span></code></a><a class="headerlink" href="#unsatisfied-condition-cuda-x-y" title="Permalink to this headline">¶</a></h3>
<p>Your CUDA container image is incompatible with your driver version.
Upgrade your driver or choose an <a class="reference external" href="https://hub.docker.com/r/nvidia/cuda/">image tag</a> which is supported by your driver (see also <a class="reference external" href="https://github.com/NVIDIA/nvidia-docker/wiki/CUDA#requirements">CUDA requirements</a>)</p>
</div>
<div class="section" id="i-have-multiple-gpu-devices-how-can-i-isolate-them-between-my-containers">
<h3><a class="toc-backref" href="#id19">I have multiple GPU devices, how can I isolate them between my containers?</a><a class="headerlink" href="#i-have-multiple-gpu-devices-how-can-i-isolate-them-between-my-containers" title="Permalink to this headline">¶</a></h3>
<p>GPU isolation is achieved through the CLI option <code class="docutils literal notranslate"><span class="pre">--gpus</span></code>. Devices can be referenced by index (following the PCI bus order) or by UUID.</p>
<p>e.g:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span># If you have 4 GPUs, to isolate GPUs 3 and 4 (/dev/nvidia2 and /dev/nvidia3)
$ docker run --gpus device=2,3 nvidia/cuda:9.0-base nvidia-smi
</pre></div>
</div>
</div>
<div class="section" id="why-is-nvidia-smi-inside-the-container-not-listing-the-running-processes">
<h3><a class="toc-backref" href="#id20">Why is <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> inside the container not listing the running processes?</a><a class="headerlink" href="#why-is-nvidia-smi-inside-the-container-not-listing-the-running-processes" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> and NVML are not compatible with <a class="reference external" href="http://man7.org/linux/man-pages/man7/pid_namespaces.7.html">PID namespaces</a>.
We recommend monitoring your processes on the host or inside a container using <code class="docutils literal notranslate"><span class="pre">--pid=host</span></code>.</p>
<p>The default behavior is an AND gate between host PID and GPU access:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 25%" />
<col style="width: 33%" />
<col style="width: 42%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Host PID</p></th>
<th class="head"><p>GPU Visible</p></th>
<th class="head"><p>Process Visible</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Yes</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-odd"><td><p>No</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
</tr>
<tr class="row-even"><td><p>Yes</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
</tr>
<tr class="row-odd"><td><p>No</p></td>
<td><p>No</p></td>
<td><p>No</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="why-is-my-container-slow-to-start">
<h3><a class="toc-backref" href="#id21">Why is my container slow to start?</a><a class="headerlink" href="#why-is-my-container-slow-to-start" title="Permalink to this headline">¶</a></h3>
<p>You probably need to enable <a class="reference external" href="http://docs.nvidia.com/deploy/driver-persistence/index.html">persistence mode</a> to keep the kernel modules loaded and the GPUs initialized.
The recommended way is to start the <code class="docutils literal notranslate"><span class="pre">nvidia-persistenced</span></code> daemon on your host.</p>
</div>
<div class="section" id="can-i-use-it-with-docker-in-docker-a-k-a-dind">
<h3><a class="toc-backref" href="#id22">Can I use it with Docker-in-Docker (a.k.a. DinD)?</a><a class="headerlink" href="#can-i-use-it-with-docker-in-docker-a-k-a-dind" title="Permalink to this headline">¶</a></h3>
<p>If you are running a Docker client inside a container: simply mount the Docker socket and proceed as usual.
If you are running a Docker daemon inside a container: this case is untested.</p>
</div>
<div class="section" id="why-is-my-application-inside-the-container-slow-to-initialize">
<h3><a class="toc-backref" href="#id23">Why is my application inside the container slow to initialize?</a><a class="headerlink" href="#why-is-my-application-inside-the-container-slow-to-initialize" title="Permalink to this headline">¶</a></h3>
<p>Your application was probably not compiled for the compute architecture of your GPU and thus the driver must <a class="reference external" href="https://devblogs.nvidia.com/parallelforall/cuda-pro-tip-understand-fat-binaries-jit-caching/">JIT</a> all the CUDA kernels from PTX.
In addition to a slow start, the JIT compiler might generate less efficient code than directly targeting your compute architecture (see also <a class="reference external" href="#does-it-have-a-performance-impact-on-my-gpu-workload">performance impact</a>).</p>
</div>
<div class="section" id="is-the-jit-cache-shared-between-containers">
<h3><a class="toc-backref" href="#id24">Is the JIT cache shared between containers?</a><a class="headerlink" href="#is-the-jit-cache-shared-between-containers" title="Permalink to this headline">¶</a></h3>
<p>No. You would have to handle this manually with <a class="reference external" href="https://docs.docker.com/engine/admin/volumes/volumes/">Docker volumes</a>.</p>
</div>
<div class="section" id="what-is-causing-the-cuda-invalid-device-function-error">
<h3><a class="toc-backref" href="#id25">What is causing the CUDA <code class="docutils literal notranslate"><span class="pre">invalid</span> <span class="pre">device</span> <span class="pre">function</span></code> error?</a><a class="headerlink" href="#what-is-causing-the-cuda-invalid-device-function-error" title="Permalink to this headline">¶</a></h3>
<p>Your application was not compiled for the compute architecture of your GPU, and no PTX was generated during build time. Thus, JIT compiling is impossible (see also <a class="reference external" href="#why-is-my-application-inside-the-container-slow-to-initialize">slow to initialize</a>).</p>
</div>
<div class="section" id="why-do-i-get-insufficient-permissions-for-some-nvidia-smi-operations">
<h3><a class="toc-backref" href="#id26">Why do I get <code class="docutils literal notranslate"><span class="pre">Insufficient</span> <span class="pre">Permissions</span></code> for some <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code> operations?</a><a class="headerlink" href="#why-do-i-get-insufficient-permissions-for-some-nvidia-smi-operations" title="Permalink to this headline">¶</a></h3>
<p>Some device management operations require extra privileges (e.g. setting clocks frequency).
After learning about the security implications of doing so, you can add extra <a class="reference external" href="https://docs.docker.com/engine/security/security/#linux-kernel-capabilities">capabilities</a> to your container using <code class="docutils literal notranslate"><span class="pre">--cap-add</span></code> on the command-line (<code class="docutils literal notranslate"><span class="pre">--cap-add=SYS_ADMIN</span></code> will allow most operations).</p>
</div>
<div class="section" id="can-i-profile-and-debug-my-gpu-code-inside-a-container">
<h3><a class="toc-backref" href="#id27">Can I profile and debug my GPU code inside a container?</a><a class="headerlink" href="#can-i-profile-and-debug-my-gpu-code-inside-a-container" title="Permalink to this headline">¶</a></h3>
<p>Yes but as stated above, you might need extra privileges, meaning extra <a class="reference external" href="https://docs.docker.com/engine/security/security/#linux-kernel-capabilities">capabilities</a> like <code class="docutils literal notranslate"><span class="pre">CAP_SYS_PTRACE</span></code> or tweak the <a class="reference external" href="https://docs.docker.com/engine/security/seccomp/">seccomp profile</a> used by Docker to allow certain syscalls.</p>
</div>
</div>
<div class="section" id="container-images">
<h2><a class="toc-backref" href="#id28">Container images</a><a class="headerlink" href="#container-images" title="Permalink to this headline">¶</a></h2>
<div class="section" id="what-do-i-have-to-install-in-my-container-images">
<h3><a class="toc-backref" href="#id29">What do I have to install in my container images?</a><a class="headerlink" href="#what-do-i-have-to-install-in-my-container-images" title="Permalink to this headline">¶</a></h3>
<p>Library dependencies vary from one application to another. In order to make things easier for developers, we provide a set of <a class="reference external" href="#do-you-provide-official-docker-images">official images</a> to base your images on.</p>
</div>
<div class="section" id="do-you-provide-official-docker-images">
<h3><a class="toc-backref" href="#id30">Do you provide official Docker images?</a><a class="headerlink" href="#do-you-provide-official-docker-images" title="Permalink to this headline">¶</a></h3>
<p>Yes, container images are available on <a class="reference external" href="https://github.com/NVIDIA/nvidia-docker/wiki/Docker-Hub">Docker Hub</a> and on the <a class="reference external" href="https://github.com/NVIDIA/nvidia-docker/wiki/NGC">NGC registry</a>.</p>
</div>
<div class="section" id="can-i-use-the-gpu-during-a-container-build-i-e-docker-build">
<h3><a class="toc-backref" href="#id31">Can I use the GPU during a container build (i.e. <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">build</span></code>)?</a><a class="headerlink" href="#can-i-use-the-gpu-during-a-container-build-i-e-docker-build" title="Permalink to this headline">¶</a></h3>
<p>Yes, as long as you <a class="reference external" href="https://github.com/NVIDIA/nvidia-docker/wiki/Advanced-topics#default-runtime">configure your Docker daemon</a> to use the <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> runtime as the default, you will be able to have build-time GPU support. However, be aware that this can render your images non-portable (see also <a class="reference external" href="#what-is-causing-the-cuda-invalid-device-function-error">invalid device function</a>).</p>
</div>
<div class="section" id="are-my-container-images-built-for-version-1-0-compatible-with-2-0-and-3-0">
<h3><a class="toc-backref" href="#id32">Are my container images built for version 1.0 compatible with 2.0 and 3.0?</a><a class="headerlink" href="#are-my-container-images-built-for-version-1-0-compatible-with-2-0-and-3-0" title="Permalink to this headline">¶</a></h3>
<p>Yes, for most cases. The main difference being that we don’t mount all driver libraries by default in 2.0 and 3.0. You might need to set the <code class="docutils literal notranslate"><span class="pre">CUDA_DRIVER_CAPABILITIES</span></code> environment variable in your Dockerfile or when starting the container. Check the documentation of <a class="reference external" href="https://github.com/nvidia/nvidia-container-runtime#environment-variables-oci-spec">nvidia-container-runtime</a>.</p>
</div>
<div class="section" id="how-do-i-link-against-driver-apis-at-build-time-e-g-libcuda-so-or-libnvidia-ml-so">
<h3><a class="toc-backref" href="#id33">How do I link against driver APIs at build time (e.g. <code class="docutils literal notranslate"><span class="pre">libcuda.so</span></code> or <code class="docutils literal notranslate"><span class="pre">libnvidia-ml.so</span></code>)?</a><a class="headerlink" href="#how-do-i-link-against-driver-apis-at-build-time-e-g-libcuda-so-or-libnvidia-ml-so" title="Permalink to this headline">¶</a></h3>
<p>Use the library stubs provided in <code class="docutils literal notranslate"><span class="pre">/usr/local/cuda/lib64/stubs/</span></code>. Our official images already take care of setting <cite>``LIBRARY_PATH`</cite> &lt;<a class="reference external" href="https://gitlab.com/nvidia/cuda/blob/ubuntu16.04/9.0/devel/Dockerfile#L12">https://gitlab.com/nvidia/cuda/blob/ubuntu16.04/9.0/devel/Dockerfile#L12</a>&gt;`_.
However, do not set <code class="docutils literal notranslate"><span class="pre">LD_LIBRARY_PATH</span></code> to this folder, the stubs must not be used at runtime.</p>
</div>
<div class="section" id="the-official-cuda-images-are-too-big-what-do-i-do">
<h3><a class="toc-backref" href="#id34">The official CUDA images are too big, what do I do?</a><a class="headerlink" href="#the-official-cuda-images-are-too-big-what-do-i-do" title="Permalink to this headline">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">devel</span></code> <a class="reference external" href="https://hub.docker.com/r/nvidia/cuda/">image tags</a> are large since the CUDA toolkit ships with many libraries, a compiler and various command-line tools.
As a general rule of thumb, you shouldn’t ship your application with its build-time dependencies. We recommend to use <a class="reference external" href="https://docs.docker.com/engine/userguide/eng-image/multistage-build/">multi-stage builds</a> for this purpose. Your final container image should use our <code class="docutils literal notranslate"><span class="pre">runtime</span></code> or <code class="docutils literal notranslate"><span class="pre">base</span></code> images.
As of CUDA 9.0 we now ship a <code class="docutils literal notranslate"><span class="pre">base</span></code> <a class="reference external" href="https://hub.docker.com/r/nvidia/cuda/">image tag</a> which bundles the strict minimum of dependencies.</p>
</div>
<div class="section" id="why-aren-t-cuda-10-images-working-with-nvidia-docker-v1">
<h3><a class="toc-backref" href="#id35">Why aren’t CUDA 10 images working with nvidia-docker v1?</a><a class="headerlink" href="#why-aren-t-cuda-10-images-working-with-nvidia-docker-v1" title="Permalink to this headline">¶</a></h3>
<p>Starting from CUDA 10.0, the CUDA images require using nvidia-docker v2 and won’t trigger the GPU enablement path from nvidia-docker v1.</p>
</div>
</div>
<div class="section" id="ecosystem-enablement">
<h2><a class="toc-backref" href="#id36">Ecosystem enablement</a><a class="headerlink" href="#ecosystem-enablement" title="Permalink to this headline">¶</a></h2>
<div class="section" id="do-you-support-docker-swarm-mode">
<h3><a class="toc-backref" href="#id37">Do you support Docker Swarm mode?</a><a class="headerlink" href="#do-you-support-docker-swarm-mode" title="Permalink to this headline">¶</a></h3>
<p>Not currently, support for Swarmkit is still being worked on in the upstream Moby project. You can track our progress <a class="reference external" href="https://github.com/moby/moby/issues/33439">on gitub here</a>.</p>
</div>
<div class="section" id="do-you-support-docker-compose">
<h3><a class="toc-backref" href="#id38">Do you support Docker Compose?</a><a class="headerlink" href="#do-you-support-docker-compose" title="Permalink to this headline">¶</a></h3>
<p>Yes, use Compose format <code class="docutils literal notranslate"><span class="pre">2.3</span></code> and add <code class="docutils literal notranslate"><span class="pre">runtime:</span> <span class="pre">nvidia</span></code> to your GPU service. Docker Compose must be version <a class="reference external" href="https://github.com/docker/compose/releases/tag/1.19.0">1.19.0</a> or higher. You can find an example <a class="reference external" href="https://github.com/NVIDIA/gpu-monitoring-tools/blob/master/exporters/prometheus-dcgm/docker/docker-compose.yml">here</a>.
Note that you’ll have to install the old <code class="docutils literal notranslate"><span class="pre">nvidia-docker2</span></code> packages.</p>
</div>
<div class="section" id="do-you-support-kubernetes">
<h3><a class="toc-backref" href="#id39">Do you support Kubernetes?</a><a class="headerlink" href="#do-you-support-kubernetes" title="Permalink to this headline">¶</a></h3>
<p>Since Kubernetes 1.8, the recommended way is to use our official <a class="reference external" href="https://github.com/NVIDIA/k8s-device-plugin">device plugin</a>.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="platform.html" class="btn btn-neutral float-right" title="Platform support Information" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="advanced-usage.html" class="btn btn-neutral float-left" title="Advanced Usage" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, NVIDIA Corporation

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>